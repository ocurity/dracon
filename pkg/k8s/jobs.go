package k8s

import (
	"context"
	"fmt"
	"io"
	"strings"
	"sync"
	"time"

	"github.com/go-errors/errors"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/apimachinery/pkg/watch"
	v1 "k8s.io/client-go/kubernetes/typed/core/v1"
)

var ErrJobDeletedWhileWatching = errors.New("job was deleted while watching it")

// WatchJobPodLogs looks for a job and streams all the logs of the pods generated by the job until
// either one of the resulting containers suceeds or the timeout kicks in.
func WatchJobPodLogs(ctx context.Context, client ClientInterface, namespace, name string, interval uint, out io.Writer) error {
	// get the job so that we can use it's label selector for subsequent pod queries
	deployedJob, err := client.
		BatchV1().
		Jobs(namespace).
		Get(ctx, name, metav1.GetOptions{})
	if err != nil {
		return fmt.Errorf("%s/%s: could not get Job: %w", namespace, name, err)
	}

	// start a watcher to monitor the job status
	watchOpts := metav1.SingleObject(deployedJob.ObjectMeta)
	watchOpts.Watch = true
	watcher, err := client.
		BatchV1().
		Jobs(namespace).
		Watch(ctx, watchOpts)
	if err != nil {
		return fmt.Errorf("could not watch status of migration job: %w", err)
	}
	defer watcher.Stop()

	podWatchingCtx, cancel := context.WithCancelCause(ctx)

	go func() {
		for {
			select {
			case event := <-watcher.ResultChan():
				if event.Type == watch.Deleted {
					// output that the job was deleted
					// send a message if the Job itself is deleted
					cancel(fmt.Errorf("%s/%s: %v", namespace, name, ErrJobDeletedWhileWatching))
					return
				}
			case <-ctx.Done():
				return
			}
		}
	}()

	// build field selectors to find pods related to the Job
	fieldSelectorSB := strings.Builder{}
	// remove from the list Pending pods or pods whose state is Unknown
	fieldSelectorSB.WriteString("status.phase!=Pending,status.phase!=Unknown")
	listSelection := metav1.ListOptions{
		LabelSelector: metav1.FormatLabelSelector(deployedJob.Spec.Selector),
		FieldSelector: fieldSelectorSB.String(),
	}

	// waitgroup to wait for all logging output to complete
	wg := &sync.WaitGroup{}
	wg.Add(1)
	// client for all the pods in the namespace
	podClient := client.CoreV1().Pods(namespace)
	// start the goroutine to watch the logs and statuses of all the pods that
	// are generated
	go watchPodLogsAndStatuses(podWatchingCtx, cancel, podClient, wg, listSelection, interval, out)

	// wait for all the pods to finish streaming logs or everything to return
	// after a cancelled/timed out context
	wg.Wait()

	return podWatchingCtx.Err()
}

// watchPodLogsAndStatuses polls the APi for new pods to setup log streaming
func watchPodLogsAndStatuses(
	ctx context.Context,
	cancel context.CancelCauseFunc,
	podClient v1.PodInterface,
	wg *sync.WaitGroup,
	listOpts metav1.ListOptions,
	interval uint,
	out io.Writer,
) {
	defer wg.Done()

	podLogsStreaming := sets.NewString()
	podsFilteredOut := sets.NewString()
	
	for {
		podList, err := podClient.List(ctx, listOpts)
		if err != nil {
			// communicate this error via the context.
			cancel(fmt.Errorf("could not list pods labels %s: %w", listOpts.LabelSelector, err))
			return
		}

		for _, pod := range podList.Items {
			if !podLogsStreaming.Has(pod.Name) {
				wg.Add(1)
				go streamPodLogs(ctx, pod.Namespace, pod.Name, podClient, wg, out)
				podLogsStreaming.Insert(pod.Name)
			}

			if pod.Status.Phase == corev1.PodSucceeded {
				return
			} else if pod.Status.Phase == corev1.PodFailed && !podsFilteredOut.Has(pod.Name) {
				// don't check the state of the pod again if it failed. its
				// logs have already been streamed.
				listOpts.LabelSelector += fmt.Sprintf(",metadata.name!=%s", pod.Name)
				podsFilteredOut.Insert(pod.Name)
			}
		}

		if len(podList.Items) == 0 {
			if _, err = fmt.Fprintf(out, "found no running/succeeded/failed pods, waiting for %ds to list again\n", interval); err != nil {
				panic(err)
			}
		}

		select {
		case <-ctx.Done():
			return
		case <-time.After(time.Duration(interval) * time.Second):
		}
	}
}

func streamPodLogs(ctx context.Context, namespace, name string, podClient v1.PodInterface, wg *sync.WaitGroup, out io.Writer) {
	defer wg.Done()

	stream, err := podClient.
		GetLogs(name, &corev1.PodLogOptions{Follow: true}).
		Stream(ctx)
	if err != nil {
		if _, err = fmt.Fprintf(out, "%s/%s: could not stream logs: %v", namespace, name, err); err != nil {
			panic(err)
		}
		return
	}

	_, err = io.Copy(out, stream)
	sErr := stream.Close()
	if sErr != nil {
		if err != nil {
			err = fmt.Errorf("%w: %w", sErr, err)
		} else {
			err = sErr
		}
	}

	if err != nil {
		if _, err = fmt.Fprintf(out, "%s/%s: could not stream logs: %v", namespace, name, err); err != nil {
			panic(err)
		}
	}

	return
}
