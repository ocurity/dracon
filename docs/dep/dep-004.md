# Extending our Helm integration to allow more dynamic discovery of tasks

## Introduction

We have recently started seeing a new type of issue: users get an error when
they try to deploy more than one pipelines. The way that pipelines get deployed
, according to our own documentation is using Helm. Many of the pipelines, if
not all of them, share some tasks which causes Helm to try to deploy the same
Task object for more than one pipelines. This is not allowed, since it would
cause the same object to be either overwritten by a different package or it
would cause it to be owned by multiple packages, which would break a lot of
Helm's assumptions. We need to rethink how we manageour Tasks distribution.

## Design

Helm is the standard tool for managing Kuberenetes objects, and it has a lot of
very useful features that would be difficult for us to replicate. It would be
preferrable if we could keep using Helm, but in a more appropriate way. At the
moment we are tightly coupling the definition of a workflow with the deployment
of its resources. Specifically, we tend to produce a manifest that contains
both the Tasks AND the Pipeline definition. However, Tasks and Pipelines are
fundamentally different. Tasks contain container image URLs, for example, or
they could have various information related to the way that secrets and volumes
or limits are applied in the cluster where the workflow is expected to run. A
Pipeline definition should be the exact same between different cluster/
environments/organisations since it's just a combination of Tasks to produce a
result.

So our goal should be to modify and extend our code to have a clearer
separation between the operations and phases of the lifetime of the system.

The first stage of the lifetime of the system consists of the deployment of
Tasks on the clusterand that should be achieved through Helm packages, allowing
the system operator to deploy and modify the configuration in whatever way they
deem necessary.

An example of why we need to do this is a security teams who wishes to have a
custom base image forall their Tasks, which requires them to build all the
images from scratch and push them into aprivate container registry. The host of
the registry is going to be supplied as a parameter to Helm during deployment,
but it should not be required when deploying a Pipeline. A Pipeline that
discovers with a high probability vulnerabilities in a Javascript codebase
should be able to run inany cluster as long as the Tasks are available.
Packaging all the Tasks into one Helm packagemakes this trivial to achieve,
since it can be easily uploaded and distributed to an OCI registry.

The second stage begins as soon as the `draconctl` is invoked to deploy a
Pipeline. The Pipeline lists a set of components. The user has 3 different
formats to describe a component:

1. a local filesystem path

This is used by the system to, on the fly read a Task and deploy it on the
cluster. There are not going to be any checks if the Task already exists, or if
the version of the Task is newer than the Task being fetched. It's going to be
`applied` on the cluster, as if the user is invoking `kubectl apply ...`.

2. a URL

The URL is going to be resolved and the Task is going to be fetched and
deployed on the cluster similar to the case where we have a file in the local
filesystem.

3. a pURL-inspired id of a Task deployed using Helm

As mentioned previously, we wish to package all our Tasks in one Helm package
that can be easily deployed on a cluster and provides access to all our
components. When a user wants to reference a Task deployed via Helm on the
cluster in a Pipeline description, they will have to use a new URI type that is
inspired by `pURL` and looks like this: `helm://{{ helm_release_name }}/name`.

The `helm_release_name` is the name of the release chosen by the operator when
deploying the package, so it's specific to the deployment context. The name of
the Task will be the actual name of the object. The system will discover all
the Tasks belonging to the Helm package by filtering with the label
`app.kubernetes.io/managed-by: Helm` and then the label
`app.kubernetes.io/instance: { helm_release_name }` and will expect to find the
Task. If the Task is not found, an error will be returned.

## Implementation

A new interface will be added to the system that can be used for an actual
implementation but also for mocks for testing purposes:

```golang
type Applier struct {
    Apply()
}

type Component struct {
    ID() string

    TektonV1Beta1Task() (*tektonv1beta1api.Task, error)

    ManagedByHelm() bool
}

type Orchestrator interface {
    Prepare([]Component) error

    Deploy(Applier) error
}
```

The `Prepare` method takes as an argument the list of Tasks that are part of
the Pipeline and ensures that they are deployed. If the Tasks are managed by
Helm it will make sure that they are available, if they are local or remote
files, they will be applied on the cluster. If no error is returned the client
can invoke the `Deploy` method which will deploy the Pipeline itself on the
cluster.

It is important to note, that the Orchestrator does not manage the assembling
of a Pipeline or the validation. It expects to receive a correct object and
will try to deploy whatever pipeline isprovided.

The `Component` struct is an abstraction layer on top of any component we are
using. It will internally resolve the Task URI and figure out how to manage it,
such as getting a FileLoaderinstance that fetches the file into memory and
parses it. If it's managing a Helm-managed Task, it will return an error if the
`TektonV1Beta1Task` is invoked, otherwise it will return the Task itself.

### Implementation Details

Since this is going to be a large piece of work it is an opportunity for us to
fix the naming of some of our packages. The `pkg/components` package which
currently only contains an enum of thetypes of components that we define, can
be merged into the `pkg/pipelines` package. The`Component` struct and the
`Orchestrator` interface will be also placed in the `pkg/pipelines` package.

We also need to introduce a new package called `pkg/k8s` with an interface
called `Applier` that replicates the behaviour of the `kubectl apply` command
along with a mock implementation of the interface for testing purposes.

We will also need to introduce a new commands to the `draconctl` for packaging
the tasks and deploying the pipelines. The new commands will be the following:

```bash
draconctl componentns package [components_path]
draconctl pipelines deploy [path to a kustomization description file]
```

The `draconctl pipelines build` command can be retired and used as a base for
the `deploy` command. A `--dry-run` flag can be added so that the `Pipeline`
flag can be written to some `io.Writer`. A `--only-validate` command can be
used to check the state of the cluster without trying to deploy the Pipeline at
all.
