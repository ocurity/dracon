# Move away from kustomize-component-generator

## Introduction

While refactoring the repository to move away from `please` we hit a snag: our pipeline generation
started failing and we couldn't figure out why. While looking into it and thinking more about the
way things are generated we realised that we had chosen a method of producing our pipelines that
was introduced more issues than it solved. So we decided to move away from
`kustomize-component-generator` and to build our own tool that can be extended in arbitrary ways.

## Description

The way `kustomize-component-generator` works is that it picks up the `task.yaml` of each component
and based on that it produces a `kustomize.yaml` manifest containing the description of a
`kustomize` component. `kustomize` components are pieces of configuration for Kubernetes that are
supposed to be self-contained and can be added/removed from a base manifest. Looking closely at the
examples provided by the `kustomize` docs, we can see that the point of these components is that
they must not have any dependencies between them.

These assumptions about how `kustomize` components are expected to be used are conflicting with our
assumptions about how tasks are supposed to work. Because the tasks that are part of a pipeline are
expected to have various interactions. Producers are expected to wait for the source task to finish
pulling the code, while the `producer-aggregator` is expected to wait for an arbitrary number of
producer tasks to finish so that their results can be gathered. Enrichers are also supposed to wait
for the `producer-aggregator` to produce its result before starting. Having an arbitrary amount of
tasks whose existence influences the setup of an arbitrary number of other tasks is not what
`kustomize` components where meant to solve. In order for our pipelines to be produced without
issues we need to have a tool that first gathers all tasks and is able to process the full context
of the pipeline before producing the actual manifest. Such a tool is not actually hard to create
since the business logic is fairly straightforward: all the tasks are sorted based on their type
and then added to the pipeline in order.

The component types are the following:

1. base
2. source
3. producer
4. producer-aggregator
5. enricher
6. enrigher-aggregator
7. consumer

Each type listed previously needs to wait for the component type before it to finish its work
before starting its own. This setup is very simple to implement with a completely custom tool.

The tool will parse the same `kustomization.yaml` files that are already present in our examples.
It's not expected to be able to parse all the potential fields of the `Kustomize` CRD, just the
basic ones that we already use. More specifically it's expected to parse:

1. nameSuffix: a suffix added to the Tekton pipeline instance name
2. namePrefix: a prefix added to the Tekton pipeline instance name
3. resources: a list of base resources that will be used to build the Pipeline. we expecte exactly
two items to be present in this list in any order: the base Pipeline and the base Task objects.
4. components: the list of Tasks that will be added to the Pipeline

Each entry in the components list could be a path of a folder on the local filesystem, a files on
the local filesystem or they can be URIs to files that will be downloaded. In the cases of files,
we expect them to be YAML manifests containing exactly one Tekton Task. If the entry is a path of a
folder on the local filesystem, we expect to find a YAML manifest called `task.yaml` inside the
folder.

All the Tasks listed need to have a unique name and all their parameters need to be prefixed with
the name of the Task. They should not have a namespace value set, we expect that to be set during
the deployment of a pipeline. Each task must also have a label with key
`v1.dracon.ocurity.com/component` and value one of the components listed previously.

A pipeline kustomization is free to list an arbitrary amount of components. However, the way that
these components will be executed is fixed for the time being.

The tool is expected to receive a positional argument that is either the path of the pipeline
`kustomization.yaml` or a folder containing one. The output of the tool can either be dumped in
`/dev/stdout` or in any file. The path of the file needs to be provided using the `--out` flag.

## Further considerations

This functionality will be the first feature of our `draconctl` tool, a CLI tool that will help
administrators manage `Dracon` using one binary instead of a hodgepodge of binaries.

Looking ahead into the future, introducing this tool allows us to become a lot more flexible
regarding what kind of backend we use for the execution of our pipeline. At the moment, we are a
layer built on top of Tekton. However, this does not have to be the case in the future. We could
add different backends to this tool, each one producing output for a different orchestration
platform. For example, we could create Pipelines that use Argo CD or something that is not K8s
related at all, such as Github actions. Furthermore, we need to be able to perform some checks for
our pipelines, such as making sure that the pipeline is a DAG and has no cycles. This is not
possible to do with the current structure of the input file, but we could also choose to use a
custom format for describing pipelines in the future that gives more flexibility to users to mix
and match components. That would also be possible if we stuck with the
`kustomize-component-generator`.

## Backwards Compatibility

For the time being, we could still keep using the `kustomize-component-generator` until we are
absolutely certain that the new tool is ready to be used in production. As long as the component
`kustomization.yaml` files remain in the repo the old generator will have no problem fetching them
and producing a pipeline.

## Future actions

Some time in the future we can completely remove the `kustomization.yaml` files from all the
components. Another thing we should consider in the future, is to move away from using
`kustomization.yaml` fils for describing our Pipelines and come up with our own DSL that will give
us more flexibility into how our components could be combined.
